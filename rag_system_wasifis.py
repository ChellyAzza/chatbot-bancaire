"""
Syst√®me RAG complet utilisant la base wasifis/bank-assistant-qa
"""

import os
import json
import numpy as np
from typing import List, Dict, Tuple
from pathlib import Path

# RAG dependencies
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.schema import Document
import chromadb
from sentence_transformers import SentenceTransformer

# Ollama integration
import ollama

class WasifisRAGSystem:
    """Syst√®me RAG utilisant les donn√©es wasifis/bank-assistant-qa"""
    
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.ollama_client = ollama.Client()
        self.documents = []
        
    def load_wasifis_data(self):
        """Charge les donn√©es wasifis/bank-assistant-qa pr√©par√©es"""
        print("=== Chargement des donn√©es wasifis/bank-assistant-qa ===")
        
        try:
            # Charger les donn√©es depuis processed_data
            data_files = {
                'train': 'processed_data/train.json',
                'validation': 'processed_data/validation.json',
                'test': 'processed_data/test.json'
            }
            
            all_data = []
            
            for split_name, file_path in data_files.items():
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = [json.loads(line) for line in f]
                    
                    print(f"‚úÖ {split_name}: {len(data)} exemples")
                    all_data.extend(data)
                else:
                    print(f"‚ö†Ô∏è Fichier non trouv√©: {file_path}")
            
            print(f"üìä Total: {len(all_data)} exemples charg√©s")
            
            # Convertir en documents pour RAG
            documents = []
            for i, item in enumerate(all_data):
                # Cr√©er un document structur√©
                if 'input' in item and 'output' in item:
                    content = f"Question: {item['input']}\nR√©ponse: {item['output']}"
                    
                    doc = Document(
                        page_content=content,
                        metadata={
                            "source": "wasifis/bank-assistant-qa",
                            "doc_id": i,
                            "question": item['input'],
                            "answer": item['output'],
                            "type": "qa_pair"
                        }
                    )
                    documents.append(doc)
            
            self.documents = documents
            print(f"‚úÖ {len(documents)} documents cr√©√©s pour RAG")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur de chargement: {e}")
            return False
    
    def setup_embeddings(self):
        """Configure le syst√®me d'embeddings"""
        print("\n=== Configuration des embeddings ===")
        
        try:
            # Utiliser un mod√®le d'embedding fran√ßais
            model_name = "sentence-transformers/all-MiniLM-L6-v2"
            
            print(f"Chargement du mod√®le: {model_name}")
            self.embeddings = HuggingFaceEmbeddings(
                model_name=model_name,
                model_kwargs={'device': 'cpu'},  # Utiliser CPU pour la compatibilit√©
                encode_kwargs={'normalize_embeddings': True}
            )
            
            print("‚úÖ Mod√®le d'embedding charg√©")
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur d'embedding: {e}")
            return False
    
    def create_vector_store(self):
        """Cr√©e la base de donn√©es vectorielle"""
        print("\n=== Cr√©ation de la base vectorielle ===")
        
        if not self.documents or not self.embeddings:
            print("‚ùå Documents ou embeddings non disponibles")
            return False
        
        try:
            # Cr√©er le r√©pertoire pour ChromaDB
            persist_directory = "./chroma_db_wasifis"
            
            # Diviser les documents en chunks plus petits si n√©cessaire
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""]
            )
            
            print("Division des documents en chunks...")
            split_docs = text_splitter.split_documents(self.documents)
            print(f"‚úÖ {len(split_docs)} chunks cr√©√©s")
            
            # Cr√©er la base vectorielle avec ChromaDB
            print("Cr√©ation de la base vectorielle (cela peut prendre du temps)...")
            self.vectorstore = Chroma.from_documents(
                documents=split_docs,
                embedding=self.embeddings,
                persist_directory=persist_directory
            )
            
            print(f"‚úÖ Base vectorielle cr√©√©e: {persist_directory}")
            print(f"üìä {len(split_docs)} documents index√©s")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur de cr√©ation vectorielle: {e}")
            return False
    
    def search_similar_documents(self, query: str, k: int = 3) -> List[Document]:
        """Recherche les documents similaires √† la requ√™te"""
        if not self.vectorstore:
            print("‚ùå Base vectorielle non initialis√©e")
            return []
        
        try:
            # Recherche de similarit√©
            similar_docs = self.vectorstore.similarity_search(
                query=query,
                k=k
            )
            
            return similar_docs
            
        except Exception as e:
            print(f"‚ùå Erreur de recherche: {e}")
            return []
    
    def generate_rag_response(self, user_question: str) -> Dict:
        """G√©n√®re une r√©ponse RAG compl√®te"""
        print(f"\nüîç Question: {user_question}")
        
        # 1. Rechercher des documents pertinents
        print("Recherche de documents pertinents...")
        relevant_docs = self.search_similar_documents(user_question, k=3)
        
        if not relevant_docs:
            print("‚ùå Aucun document pertinent trouv√©")
            return {
                "answer": "Je n'ai pas trouv√© d'informations pertinentes pour r√©pondre √† votre question.",
                "sources": [],
                "confidence": 0.0
            }
        
        print(f"‚úÖ {len(relevant_docs)} documents trouv√©s")
        
        # 2. Construire le contexte
        context_parts = []
        sources = []
        
        for i, doc in enumerate(relevant_docs):
            context_parts.append(f"Document {i+1}:\n{doc.page_content}")
            sources.append({
                "doc_id": doc.metadata.get("doc_id", i),
                "question": doc.metadata.get("question", ""),
                "answer": doc.metadata.get("answer", ""),
                "content": doc.page_content[:200] + "..."
            })
        
        context = "\n\n".join(context_parts)
        
        # 3. Cr√©er le prompt RAG
        rag_prompt = f"""Vous √™tes un assistant bancaire expert. Utilisez les informations suivantes pour r√©pondre √† la question du client.

CONTEXTE PERTINENT:
{context}

QUESTION DU CLIENT: {user_question}

INSTRUCTIONS:
- R√©pondez de mani√®re pr√©cise et professionnelle
- Utilisez uniquement les informations du contexte fourni
- Si le contexte ne contient pas assez d'informations, dites-le clairement
- Soyez concis mais complet

R√âPONSE:"""
        
        # 4. G√©n√©rer la r√©ponse avec Ollama
        try:
            print("G√©n√©ration de la r√©ponse avec llama3.1:8b...")
            response = self.ollama_client.chat(
                model='llama3.1:8b',
                messages=[
                    {
                        'role': 'user',
                        'content': rag_prompt
                    }
                ],
                options={
                    'temperature': 0.7,
                    'max_tokens': 512
                }
            )
            
            answer = response['message']['content']
            
            return {
                "answer": answer,
                "sources": sources,
                "context_used": context,
                "confidence": 0.8  # Score de confiance basique
            }
            
        except Exception as e:
            print(f"‚ùå Erreur de g√©n√©ration: {e}")
            return {
                "answer": f"Erreur lors de la g√©n√©ration de la r√©ponse: {e}",
                "sources": sources,
                "confidence": 0.0
            }
    
    def test_rag_system(self):
        """Teste le syst√®me RAG avec des questions"""
        print("\n=== Test du syst√®me RAG ===")
        
        test_questions = [
            "Quels sont les frais de tenue de compte?",
            "Comment ouvrir un compte √©pargne?",
            "Quelles sont les conditions pour un pr√™t immobilier?",
            "Comment activer ma carte bancaire?",
            "Que faire en cas de perte de carte?"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\n{'='*60}")
            print(f"TEST {i}: {question}")
            print('='*60)
            
            result = self.generate_rag_response(question)
            
            print(f"ü§ñ R√©ponse RAG:")
            print(result["answer"])
            
            print(f"\nüìö Sources utilis√©es:")
            for j, source in enumerate(result["sources"], 1):
                print(f"  {j}. Q: {source['question'][:100]}...")
                print(f"     R: {source['answer'][:100]}...")
            
            print(f"\nüìä Confiance: {result['confidence']:.1%}")
    
    def save_rag_config(self):
        """Sauvegarde la configuration RAG"""
        config = {
            "system": "RAG with wasifis/bank-assistant-qa",
            "total_documents": len(self.documents),
            "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
            "vector_store": "ChromaDB",
            "llm_model": "llama3.1:8b (Ollama)",
            "persist_directory": "./chroma_db_wasifis",
            "chunk_size": 500,
            "chunk_overlap": 50
        }
        
        with open("rag_config_wasifis.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Configuration RAG sauvegard√©e: rag_config_wasifis.json")

def main():
    """Fonction principale"""
    print("üè¶ Syst√®me RAG avec wasifis/bank-assistant-qa")
    print("=" * 50)
    
    rag_system = WasifisRAGSystem()
    
    # 1. Charger les donn√©es wasifis
    print("1. Chargement des donn√©es wasifis/bank-assistant-qa...")
    if not rag_system.load_wasifis_data():
        print("‚ùå √âchec du chargement des donn√©es")
        return
    
    # 2. Configurer les embeddings
    print("\n2. Configuration des embeddings...")
    if not rag_system.setup_embeddings():
        print("‚ùå √âchec de la configuration des embeddings")
        return
    
    # 3. Cr√©er la base vectorielle
    print("\n3. Cr√©ation de la base vectorielle...")
    if not rag_system.create_vector_store():
        print("‚ùå √âchec de la cr√©ation de la base vectorielle")
        return
    
    # 4. Sauvegarder la configuration
    rag_system.save_rag_config()
    
    # 5. Test du syst√®me
    print("\n4. Voulez-vous tester le syst√®me RAG? (y/n)")
    choice = input().lower().strip()
    
    if choice == 'y':
        rag_system.test_rag_system()
    
    print("\n" + "=" * 60)
    print("üéâ SYST√àME RAG PR√äT!")
    print("‚úÖ Base de donn√©es: wasifis/bank-assistant-qa (4,272 exemples)")
    print("‚úÖ Embeddings: sentence-transformers/all-MiniLM-L6-v2")
    print("‚úÖ Base vectorielle: ChromaDB")
    print("‚úÖ LLM: llama3.1:8b (Ollama)")
    print("\nüöÄ Pour utiliser le RAG:")
    print("from rag_system_wasifis import WasifisRAGSystem")
    print("rag = WasifisRAGSystem()")
    print("response = rag.generate_rag_response('Votre question')")

if __name__ == "__main__":
    main()
