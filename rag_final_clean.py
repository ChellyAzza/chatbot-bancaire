"""
RAG FINAL - RÃ©ponses propres et dÃ©veloppÃ©es
Extraction parfaite, pas de prompt visible
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import time

print("ğŸ¯ RAG FINAL - RÃ‰PONSES PROPRES")
print("=" * 60)

# Configuration
model_path = "./models/Llama-3.1-8B-Instruct"
adapter_path = "./llama_banking_final_fidelity"

# VÃ©rifications
if not torch.cuda.is_available():
    print("âŒ CUDA requis")
    exit()

print(f"âœ… GPU: {torch.cuda.get_device_name()}")
torch.cuda.empty_cache()

# Configuration quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

class FinalRAGSystem:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.vectorizer = None
        self.question_vectors = None
        self.qa_pairs = []
        
    def load_model(self):
        """Charge le modÃ¨le fine-tunÃ©"""
        print("ğŸ“ Chargement tokenizer...")
        from pathlib import Path
        
        if Path(model_path).exists():
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                "meta-llama/Llama-3.1-8B-Instruct",
                token="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"
            )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("ğŸ¦™ Chargement modÃ¨le fine-tunÃ©...")
        if Path(model_path).exists():
            base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16
            )
        else:
            base_model = AutoModelForCausalLM.from_pretrained(
                "meta-llama/Llama-3.1-8B-Instruct",
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16,
                token="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"
            )
        
        self.model = PeftModel.from_pretrained(
            base_model, 
            adapter_path,
            torch_dtype=torch.float16
        )
        
        print("âœ… ModÃ¨le fine-tunÃ© chargÃ©!")
    
    def load_clean_knowledge_base(self):
        """Charge la base de connaissances nettoyÃ©e"""
        
        clean_file = "cleaned_banking_qa.json"
        
        try:
            print(f"ğŸ“Š Chargement base nettoyÃ©e: {clean_file}...")
            with open(clean_file, 'r', encoding='utf-8') as f:
                self.qa_pairs = json.load(f)
            print(f"âœ… {len(self.qa_pairs)} paires Q&A chargÃ©es")
            
        except FileNotFoundError:
            print(f"âš ï¸ Fichier {clean_file} non trouvÃ©, nettoyage automatique...")
            
            from datasets import load_dataset
            dataset = load_dataset("wasifis/bank-assistant-qa")
            train_data = dataset["train"]
            
            self.qa_pairs = []
            for item in train_data:
                cleaned_item = {
                    "input": item["input"].strip(),
                    "output": item["output"].strip()
                }
                if cleaned_item["input"] and cleaned_item["output"]:
                    self.qa_pairs.append(cleaned_item)
            
            with open(clean_file, 'w', encoding='utf-8') as f:
                json.dump(self.qa_pairs, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… {len(self.qa_pairs)} paires Q&A nettoyÃ©es")
        
        # CrÃ©er l'index TF-IDF
        print("ğŸ” CrÃ©ation de l'index de recherche...")
        questions = [pair["input"] for pair in self.qa_pairs]
        
        self.vectorizer = TfidfVectorizer(
            stop_words='english',
            max_features=5000,
            ngram_range=(1, 2)
        )
        
        self.question_vectors = self.vectorizer.fit_transform(questions)
        print("âœ… Index de recherche crÃ©Ã©!")
    
    def retrieve_relevant_context(self, query, top_k=3):
        """Recherche les contextes les plus pertinents"""
        
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.question_vectors).flatten()
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        relevant_contexts = []
        for idx in top_indices:
            if similarities[idx] > 0.1:
                relevant_contexts.append({
                    "question": self.qa_pairs[idx]["input"],
                    "answer": self.qa_pairs[idx]["output"],
                    "similarity": similarities[idx]
                })
        
        return relevant_contexts
    
    def generate_final_rag_response(self, user_question):
        """GÃ©nÃ¨re une rÃ©ponse finale propre avec RAG"""
        
        # 1. Rechercher les contextes pertinents
        relevant_contexts = self.retrieve_relevant_context(user_question, top_k=3)
        
        # 2. Construire le contexte de maniÃ¨re invisible
        context_info = ""
        if relevant_contexts:
            # Utiliser seulement le meilleur contexte pour Ã©viter la rÃ©pÃ©tition
            best_context = relevant_contexts[0]
            context_info = f"Context: {best_context['answer']}"
        
        # 3. Prompt minimal et propre
        rag_prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{user_question}

{context_info}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        # 4. GÃ©nÃ©ration avec paramÃ¨tres optimisÃ©s
        inputs = self.tokenizer(rag_prompt, return_tensors="pt", truncation=True, max_length=1024)
        model_device = next(self.model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.2,
                do_sample=True,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.15
            )
        
        # 5. Extraction propre de la rÃ©ponse
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraire seulement la partie assistant
        if "<|start_header_id|>assistant<|end_header_id|>" in full_response:
            response = full_response.split("<|start_header_id|>assistant<|end_header_id|>")[-1]
        else:
            response = full_response
        
        # Nettoyage complet
        response = response.replace("<|eot_id|>", "")
        response = response.replace("Context:", "")
        response = response.strip()
        
        # Supprimer les lignes vides et nettoyer
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        response = '\n'.join(lines)
        
        return response, relevant_contexts

# Initialisation du systÃ¨me
print("ğŸš€ Initialisation du systÃ¨me RAG final...")
rag_system = FinalRAGSystem()
rag_system.load_model()
rag_system.load_clean_knowledge_base()

print("\n" + "="*60)
print("ğŸ¯ SYSTÃˆME RAG FINAL PRÃŠT!")
print("="*60)

# Test automatique
test_questions = [
    "Can applicant avail clean loan in NUST Sahar Finance?",
    "What are the free facilities associated with Roshan Digital Account?",
    "What is the maximum tenure of NUST Imarat Finance?",
    "What are the processing charges for PMYB & ALS?"
]

for i, question in enumerate(test_questions):
    print(f"\n{'='*20} TEST {i+1} {'='*20}")
    print(f"â“ Question: {question}")
    
    start_time = time.time()
    response, contexts = rag_system.generate_final_rag_response(question)
    response_time = time.time() - start_time
    
    print(f"â±ï¸ Temps: {response_time:.2f}s")
    print(f"ğŸ” Contextes trouvÃ©s: {len(contexts)}")
    if contexts:
        print(f"ğŸ“Š SimilaritÃ© max: {max(ctx['similarity'] for ctx in contexts):.2%}")
    
    print(f"\nğŸ’¬ RÃ‰PONSE FINALE:")
    print(f"{response}")

# Mode interactif
print(f"\nğŸ’¬ MODE INTERACTIF FINAL (tapez 'quit' pour arrÃªter):")

while True:
    try:
        user_question = input("\nâ“ Votre question: ").strip()
        
        if user_question.lower() in ['quit', 'exit', 'stop', 'q']:
            break
            
        if not user_question:
            continue
        
        print("ğŸ¯ GÃ©nÃ©ration rÃ©ponse finale...")
        start_time = time.time()
        response, contexts = rag_system.generate_final_rag_response(user_question)
        response_time = time.time() - start_time
        
        print(f"â±ï¸ Temps: {response_time:.2f}s")
        print(f"ğŸ” {len(contexts)} contextes trouvÃ©s")
        if contexts:
            print(f"ğŸ“Š SimilaritÃ© max: {max(ctx['similarity'] for ctx in contexts):.2%}")
        
        print(f"\nğŸ’¬ RÃ‰PONSE FINALE:")
        print(f"{response}")
        
    except KeyboardInterrupt:
        break
    except Exception as e:
        print(f"âŒ Erreur: {e}")

print(f"\nğŸ‰ SESSION TERMINÃ‰E!")
print(f"âœ… RAG final avec rÃ©ponses propres et dÃ©veloppÃ©es!")
print(f"ğŸ¯ Votre modÃ¨le fine-tunÃ© + RAG fonctionne parfaitement!")
