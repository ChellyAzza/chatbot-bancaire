"""
RAG corrigÃ© avec base de donnÃ©es nettoyÃ©e
Utilise seulement input/output, extraction de rÃ©ponse amÃ©liorÃ©e
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import time

print("ğŸ” RAG CORRIGÃ‰ - BASE NETTOYÃ‰E")
print("=" * 60)

# Configuration
model_path = "./models/Llama-3.1-8B-Instruct"
adapter_path = "./llama_banking_final_fidelity"

# VÃ©rifications
if not torch.cuda.is_available():
    print("âŒ CUDA requis")
    exit()

print(f"âœ… GPU: {torch.cuda.get_device_name()}")
torch.cuda.empty_cache()

# Configuration quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

class CleanRAGSystem:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.vectorizer = None
        self.question_vectors = None
        self.qa_pairs = []
        
    def load_model(self):
        """Charge le modÃ¨le fine-tunÃ©"""
        print("ğŸ“ Chargement tokenizer...")
        from pathlib import Path
        
        if Path(model_path).exists():
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                "meta-llama/Llama-3.1-8B-Instruct",
                token="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"
            )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("ğŸ¦™ Chargement modÃ¨le fine-tunÃ©...")
        if Path(model_path).exists():
            base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16
            )
        else:
            base_model = AutoModelForCausalLM.from_pretrained(
                "meta-llama/Llama-3.1-8B-Instruct",
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16,
                token="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"
            )
        
        self.model = PeftModel.from_pretrained(
            base_model, 
            adapter_path,
            torch_dtype=torch.float16
        )
        
        print("âœ… ModÃ¨le fine-tunÃ© chargÃ©!")
    
    def load_clean_knowledge_base(self):
        """Charge la base de connaissances nettoyÃ©e"""
        
        # Essayer de charger le fichier nettoyÃ©
        clean_file = "cleaned_banking_qa.json"
        
        try:
            print(f"ğŸ“Š Chargement base nettoyÃ©e: {clean_file}...")
            with open(clean_file, 'r', encoding='utf-8') as f:
                self.qa_pairs = json.load(f)
            print(f"âœ… {len(self.qa_pairs)} paires Q&A chargÃ©es depuis fichier nettoyÃ©")
            
        except FileNotFoundError:
            print(f"âš ï¸ Fichier {clean_file} non trouvÃ©, nettoyage automatique...")
            
            # Charger et nettoyer automatiquement
            from datasets import load_dataset
            dataset = load_dataset("wasifis/bank-assistant-qa")
            train_data = dataset["train"]
            
            self.qa_pairs = []
            for item in train_data:
                cleaned_item = {
                    "input": item["input"].strip(),
                    "output": item["output"].strip()
                }
                if cleaned_item["input"] and cleaned_item["output"]:
                    self.qa_pairs.append(cleaned_item)
            
            # Sauvegarder pour la prochaine fois
            with open(clean_file, 'w', encoding='utf-8') as f:
                json.dump(self.qa_pairs, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… {len(self.qa_pairs)} paires Q&A nettoyÃ©es et sauvegardÃ©es")
        
        # CrÃ©er l'index TF-IDF
        print("ğŸ” CrÃ©ation de l'index de recherche...")
        questions = [pair["input"] for pair in self.qa_pairs]
        
        self.vectorizer = TfidfVectorizer(
            stop_words='english',
            max_features=5000,
            ngram_range=(1, 2)
        )
        
        self.question_vectors = self.vectorizer.fit_transform(questions)
        print("âœ… Index de recherche crÃ©Ã©!")
    
    def retrieve_relevant_context(self, query, top_k=3):
        """Recherche les contextes les plus pertinents"""
        
        # Vectoriser la requÃªte
        query_vector = self.vectorizer.transform([query])
        
        # Calculer la similaritÃ© cosinus
        similarities = cosine_similarity(query_vector, self.question_vectors).flatten()
        
        # Obtenir les top_k rÃ©sultats
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        relevant_contexts = []
        for idx in top_indices:
            if similarities[idx] > 0.1:  # Seuil de pertinence
                relevant_contexts.append({
                    "question": self.qa_pairs[idx]["input"],
                    "answer": self.qa_pairs[idx]["output"],
                    "similarity": similarities[idx]
                })
        
        return relevant_contexts
    
    def generate_simple_response(self, user_question, max_length=200):
        """GÃ©nÃ¨re une rÃ©ponse simple (sans RAG)"""
        
        prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

{user_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        model_device = next(self.model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.1,
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.1
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ğŸ”§ EXTRACTION AMÃ‰LIORÃ‰E DE LA RÃ‰PONSE
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        
        # Nettoyer les artefacts
        response = response.replace("<|eot_id|>", "").strip()
        
        return response
    
    def generate_rag_response(self, user_question, max_length=300):
        """GÃ©nÃ¨re une rÃ©ponse avec RAG - VERSION CORRIGÃ‰E"""
        
        # 1. Rechercher les contextes pertinents
        relevant_contexts = self.retrieve_relevant_context(user_question, top_k=3)
        
        # 2. Construire le contexte de maniÃ¨re simple
        context_text = ""
        if relevant_contexts:
            context_text = "\n\nRelevant examples:\n"
            for ctx in relevant_contexts:
                context_text += f"Q: {ctx['question']}\nA: {ctx['answer']}\n\n"
        
        # 3. Prompt simplifiÃ© SANS instruction systÃ¨me visible
        rag_prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Based on the following examples, answer the question accurately:{context_text}

Question: {user_question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        # 4. GÃ©nÃ©ration
        inputs = self.tokenizer(rag_prompt, return_tensors="pt", truncation=True, max_length=1024)
        model_device = next(self.model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                temperature=0.2,  # AugmentÃ© pour plus de crÃ©ativitÃ©
                do_sample=True,
                top_p=0.9,  # AjoutÃ© pour meilleure gÃ©nÃ©ration
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.15  # AugmentÃ© pour Ã©viter rÃ©pÃ©titions
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ğŸ”§ EXTRACTION AMÃ‰LIORÃ‰E DE LA RÃ‰PONSE
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        
        # Nettoyer les artefacts
        response = response.replace("<|eot_id|>", "").strip()
        response = response.replace("Based on the following examples", "").strip()
        
        return response, relevant_contexts

# Initialisation du systÃ¨me
print("ğŸš€ Initialisation du systÃ¨me RAG corrigÃ©...")
rag_system = CleanRAGSystem()
rag_system.load_model()
rag_system.load_clean_knowledge_base()

print("\n" + "="*60)
print("ğŸ¯ SYSTÃˆME RAG CORRIGÃ‰ PRÃŠT!")
print("="*60)

# Test automatique de votre question
test_question = "Can applicant avail clean loan in NUST Sahar Finance?"

print(f"\nğŸ§ª TEST AUTOMATIQUE:")
print(f"â“ Question: {test_question}")

# Sans RAG
print(f"\nğŸ”¸ SANS RAG:")
start_time = time.time()
simple_response = rag_system.generate_simple_response(test_question)
simple_time = time.time() - start_time
print(f"â±ï¸ Temps: {simple_time:.2f}s")
print(f"ğŸ’¬ RÃ©ponse: {simple_response}")

# Avec RAG
print(f"\nğŸ”¹ AVEC RAG:")
start_time = time.time()
rag_response, contexts = rag_system.generate_rag_response(test_question)
rag_time = time.time() - start_time
print(f"â±ï¸ Temps: {rag_time:.2f}s")
print(f"ğŸ” Contextes trouvÃ©s: {len(contexts)}")
if contexts:
    print(f"ğŸ“Š SimilaritÃ© max: {max(ctx['similarity'] for ctx in contexts):.2%}")
print(f"ğŸ’¬ RÃ©ponse: {rag_response}")

# Mode interactif
print(f"\nğŸ’¬ MODE INTERACTIF (tapez 'quit' pour arrÃªter):")

while True:
    try:
        user_question = input("\nâ“ Votre question: ").strip()
        
        if user_question.lower() in ['quit', 'exit', 'stop', 'q']:
            break
            
        if not user_question:
            continue
        
        print("ğŸ”¹ AVEC RAG:")
        start_time = time.time()
        response, contexts = rag_system.generate_rag_response(user_question)
        response_time = time.time() - start_time
        
        print(f"â±ï¸ Temps: {response_time:.2f}s")
        print(f"ğŸ” {len(contexts)} contextes trouvÃ©s")
        if contexts:
            print(f"ğŸ“Š SimilaritÃ© max: {max(ctx['similarity'] for ctx in contexts):.2%}")
        print(f"ğŸ’¬ RÃ©ponse: {response}")
        
    except KeyboardInterrupt:
        break
    except Exception as e:
        print(f"âŒ Erreur: {e}")

print(f"\nğŸ‰ SESSION TERMINÃ‰E!")
print(f"âœ… RAG corrigÃ© avec extraction de rÃ©ponse amÃ©liorÃ©e!")
