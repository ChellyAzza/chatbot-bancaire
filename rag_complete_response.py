"""
RAG optimisÃ© pour rÃ©ponses complÃ¨tes
ParamÃ¨tres ajustÃ©s pour gÃ©nÃ©rer des rÃ©ponses plus dÃ©taillÃ©es
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import time

print("ğŸ” RAG OPTIMISÃ‰ - RÃ‰PONSES COMPLÃˆTES")
print("=" * 60)

# Configuration
model_path = "./models/Llama-3.1-8B-Instruct"
adapter_path = "./llama_banking_final_fidelity"

# VÃ©rifications
if not torch.cuda.is_available():
    print("âŒ CUDA requis")
    exit()

print(f"âœ… GPU: {torch.cuda.get_device_name()}")
torch.cuda.empty_cache()

# Configuration quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

class OptimizedRAGSystem:
    def __init__(self):
        self.tokenizer = None
        self.model = None
        self.vectorizer = None
        self.question_vectors = None
        self.qa_pairs = []
        
    def load_model(self):
        """Charge le modÃ¨le fine-tunÃ©"""
        print("ğŸ“ Chargement tokenizer...")
        from pathlib import Path
        
        if Path(model_path).exists():
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(
                "meta-llama/Llama-3.1-8B-Instruct",
                token="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"
            )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print("ğŸ¦™ Chargement modÃ¨le fine-tunÃ©...")
        if Path(model_path).exists():
            base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16
            )
        else:
            base_model = AutoModelForCausalLM.from_pretrained(
                "meta-llama/Llama-3.1-8B-Instruct",
                quantization_config=quantization_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16,
                token="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"
            )
        
        self.model = PeftModel.from_pretrained(
            base_model, 
            adapter_path,
            torch_dtype=torch.float16
        )
        
        print("âœ… ModÃ¨le fine-tunÃ© chargÃ©!")
    
    def load_clean_knowledge_base(self):
        """Charge la base de connaissances nettoyÃ©e"""
        
        clean_file = "cleaned_banking_qa.json"
        
        try:
            print(f"ğŸ“Š Chargement base nettoyÃ©e: {clean_file}...")
            with open(clean_file, 'r', encoding='utf-8') as f:
                self.qa_pairs = json.load(f)
            print(f"âœ… {len(self.qa_pairs)} paires Q&A chargÃ©es")
            
        except FileNotFoundError:
            print(f"âš ï¸ Fichier {clean_file} non trouvÃ©, nettoyage automatique...")
            
            from datasets import load_dataset
            dataset = load_dataset("wasifis/bank-assistant-qa")
            train_data = dataset["train"]
            
            self.qa_pairs = []
            for item in train_data:
                cleaned_item = {
                    "input": item["input"].strip(),
                    "output": item["output"].strip()
                }
                if cleaned_item["input"] and cleaned_item["output"]:
                    self.qa_pairs.append(cleaned_item)
            
            with open(clean_file, 'w', encoding='utf-8') as f:
                json.dump(self.qa_pairs, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… {len(self.qa_pairs)} paires Q&A nettoyÃ©es")
        
        # CrÃ©er l'index TF-IDF
        print("ğŸ” CrÃ©ation de l'index de recherche...")
        questions = [pair["input"] for pair in self.qa_pairs]
        
        self.vectorizer = TfidfVectorizer(
            stop_words='english',
            max_features=5000,
            ngram_range=(1, 2)
        )
        
        self.question_vectors = self.vectorizer.fit_transform(questions)
        print("âœ… Index de recherche crÃ©Ã©!")
    
    def retrieve_relevant_context(self, query, top_k=5):  # AugmentÃ© Ã  5 contextes
        """Recherche les contextes les plus pertinents"""
        
        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.question_vectors).flatten()
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        relevant_contexts = []
        for idx in top_indices:
            if similarities[idx] > 0.05:  # Seuil rÃ©duit pour plus de contextes
                relevant_contexts.append({
                    "question": self.qa_pairs[idx]["input"],
                    "answer": self.qa_pairs[idx]["output"],
                    "similarity": similarities[idx]
                })
        
        return relevant_contexts
    
    def generate_complete_rag_response(self, user_question):
        """GÃ©nÃ¨re une rÃ©ponse complÃ¨te avec RAG optimisÃ©"""
        
        # 1. Rechercher plus de contextes
        relevant_contexts = self.retrieve_relevant_context(user_question, top_k=5)
        
        # 2. Construire un contexte riche
        context_text = ""
        if relevant_contexts:
            context_text = "\n\nRelevant banking information:\n"
            for i, ctx in enumerate(relevant_contexts[:3]):  # Top 3 seulement
                context_text += f"Example {i+1}:\nQ: {ctx['question']}\nA: {ctx['answer']}\n\n"
        
        # 3. Prompt optimisÃ© pour rÃ©ponses complÃ¨tes
        rag_prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

You are a banking expert. Based on the following banking information, provide a complete and detailed answer to the question.{context_text}

Question: {user_question}

Please provide a comprehensive answer with all relevant details.<|eot_id|><|start_header_id|>assistant<|end_header_id|>

"""
        
        # 4. GÃ©nÃ©ration avec paramÃ¨tres optimisÃ©s
        inputs = self.tokenizer(rag_prompt, return_tensors="pt", truncation=True, max_length=1500)
        model_device = next(self.model.parameters()).device
        inputs = {k: v.to(model_device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=400,  # AugmentÃ© pour rÃ©ponses plus longues
                temperature=0.3,     # Ã‰quilibrÃ© crÃ©ativitÃ©/prÃ©cision
                do_sample=True,
                top_p=0.9,
                top_k=50,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=1.2,
                length_penalty=1.1   # Encourage les rÃ©ponses plus longues
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # 5. Extraction et nettoyage amÃ©liorÃ©s
        if "<|start_header_id|>assistant<|end_header_id|>" in response:
            response = response.split("<|start_header_id|>assistant<|end_header_id|>")[-1].strip()
        
        # Nettoyer tous les artefacts
        response = response.replace("<|eot_id|>", "").strip()
        response = response.replace("Based on the following", "").strip()
        response = response.replace("You are a banking expert.", "").strip()
        
        # Supprimer les lignes vides multiples
        lines = response.split('\n')
        cleaned_lines = []
        for line in lines:
            if line.strip():
                cleaned_lines.append(line.strip())
        
        response = '\n'.join(cleaned_lines)
        
        return response, relevant_contexts

# Initialisation du systÃ¨me
print("ğŸš€ Initialisation du systÃ¨me RAG optimisÃ©...")
rag_system = OptimizedRAGSystem()
rag_system.load_model()
rag_system.load_clean_knowledge_base()

print("\n" + "="*60)
print("ğŸ¯ SYSTÃˆME RAG OPTIMISÃ‰ PRÃŠT!")
print("="*60)

# Test de votre question spÃ©cifique
test_question = "Can applicant avail clean loan in NUST Sahar Finance?"

print(f"\nğŸ§ª TEST OPTIMISÃ‰:")
print(f"â“ Question: {test_question}")

print(f"\nğŸ”¹ RÃ‰PONSE COMPLÃˆTE AVEC RAG:")
start_time = time.time()
response, contexts = rag_system.generate_complete_rag_response(test_question)
response_time = time.time() - start_time

print(f"â±ï¸ Temps: {response_time:.2f}s")
print(f"ğŸ” Contextes trouvÃ©s: {len(contexts)}")
if contexts:
    print(f"ğŸ“Š SimilaritÃ© max: {max(ctx['similarity'] for ctx in contexts):.2%}")
    print(f"ğŸ“š Sources utilisÃ©es:")
    for i, ctx in enumerate(contexts[:3]):
        print(f"  {i+1}. SimilaritÃ©: {ctx['similarity']:.2%}")

print(f"\nğŸ’¬ RÃ‰PONSE COMPLÃˆTE:")
print(f"{response}")

# Mode interactif optimisÃ©
print(f"\nğŸ’¬ MODE INTERACTIF OPTIMISÃ‰ (tapez 'quit' pour arrÃªter):")

while True:
    try:
        user_question = input("\nâ“ Votre question: ").strip()
        
        if user_question.lower() in ['quit', 'exit', 'stop', 'q']:
            break
            
        if not user_question:
            continue
        
        print("ğŸ”¹ GÃ‰NÃ‰RATION RÃ‰PONSE COMPLÃˆTE...")
        start_time = time.time()
        response, contexts = rag_system.generate_complete_rag_response(user_question)
        response_time = time.time() - start_time
        
        print(f"â±ï¸ Temps: {response_time:.2f}s")
        print(f"ğŸ” {len(contexts)} contextes trouvÃ©s")
        if contexts:
            print(f"ğŸ“Š SimilaritÃ© max: {max(ctx['similarity'] for ctx in contexts):.2%}")
        
        print(f"\nğŸ’¬ RÃ‰PONSE COMPLÃˆTE:")
        print(f"{response}")
        
    except KeyboardInterrupt:
        break
    except Exception as e:
        print(f"âŒ Erreur: {e}")

print(f"\nğŸ‰ SESSION TERMINÃ‰E!")
print(f"âœ… RAG optimisÃ© pour rÃ©ponses complÃ¨tes et dÃ©taillÃ©es!")
